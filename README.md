# HMM_POS_tagging
## Heuristic Description

For my HMM, I created three probability tables, which are the initial probability table, transition probability tables, and the emission probability table. These are created by counting. Then, I tried to use the Viterbi algorithm to predict the tags of each sentence, but I soon realized that I have to variate from the Viterbi algorithm in order to adjust for certain cases.

One small difference between my algorithm and the Viterbi algorithm is that I did not need the “prev” to keep track of the path, because the “prob” matrix has shape (row = number of words in the sentence, column = number of tag). The index of the column followed the order in the list that consist of all tags (i.e., if I have 50 tags, there will be 50 columns, and column 1 refers to tag 1), so by finding the index of the maximum value in each row, I could acquire the prediction for each word.

There were two cases that I needed to solve using my own heuristic. The first one was when the words in the test file did not occur in the training files. In this case there is no emission probability since the words were never observed. My solution for this was to calculate the maximum probability of the tag in each position of the sentence and use it to infer the probability of the word. For example, if the new word in the test file is the first word in the sentence, then I will look up the position probability table and assign it with the first tag, because it is the tag that occur the most often as the first word in all the training sentences.

The second case, which is a follow-up case, is that when the sentences in the test file are longer than the sentences in my training files and new words are observed. In this case, my position probability table is ineffective because it will be out of index (the longest sentence when training has 10 words, so my table has 10 items; it cannot be used if I encounter a sentence with 20 new words). The solution that I originally wanted to try is to further break down those long sentences into sub sentences by splitting on punctuations like commas or semicolons. It is rare to have no punctuation inside a long sentence, and usually these long sentences have subordinate clauses that have a similar structure to the main clause. Thus, breaking a long sentence down to different shorter “sentences” and inferring tags from them can be a viable solution. However, I did not have time to finish this implementation. Instead, I randomly chose a tag from the position table for the new words in sentences that exceed the longest length.

Even though it was not recommended, I also hard coded a few tags in the case where new words were found. This is only for signs like “ ( ” and “ ] ”, or words like “ at ” and “ on ”. I did it because I think these are punctuations or prepositions that do not have any other tags that can fit them. For instance, if the program reads “ ( ” as an input and it has never learned it from the training files before, I can help it to recognize the punctuation as “PUL,” as there is no other possible solution.
